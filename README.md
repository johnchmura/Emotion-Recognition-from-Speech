# Speech Emotion & Vocalâ€‘Channel Recognition

A Python toolkit for training and evaluating **Hidden Markov Models** (HMMs) on speech data to perform:

- **Emotion Recognition** (neutral, calm, happy, sad, angry, fearful, disgust, surprised)  
- **Vocalâ€‘Channel Classification** (speech vs. song)

This package provides endâ€‘toâ€‘end data download, parsing, preprocessing, feature extraction, model training and evaluation via a single CLI entrypoint `ser`.

---

## Installation

1. **Clone the repository**  
   ```bash
   git clone https://github.com/yourâ€‘org/speech_emotion_recognition.git
   cd speech_emotion_recognition
   ```

2. **Create & activate a virtual environment**  
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies in editable mode**  
   ```bash
   pip install --upgrade pip setuptools wheel
   pip install -e .
   ```

---

## Configuration

All paths, URLs, and hyperparameters live in `config.toml` at the project root:

```toml
[paths]
raw_data_dir     = "data/Audio_Speech_Actors_01-24"
preproc_dir      = "data/preprocessed"
cleaned_csv      = "data/cleaned_data.csv"
models_dir       = "models"

[download]
url_song = "https://zenodo.org/records/1188976/files/Audio_Song_Actors_01-24.zip?download=1"
url_speech = "https://zenodo.org/records/1188976/files/Audio_Speech_Actors_01-24.zip?download=1"

[hmm]
n_states = 5
n_iter   = 30

[mfcc]
sr        = 16000
n_mfcc    = 13
frame_len = 0.025
hop_len   = 0.010
n_mels    = 40
```

Adjust any of these settings as needed.

---

## Usage

After installation, use the `ser` CLI:

```bash
ser --help
```

### 1. Download

Fetch and unzip the RAVDESS archives:

```bash
# Download both speech and song (default)
ser download

# Download only the speech dataset
ser download --type speech

# Download only the song dataset
ser download -t song
```

Files will be extracted under `paths.raw_data_dir`.

### 2. Preprocess

Parse filenames to metadata and preprocess audio:

```bash
ser preprocess
```

Outputs:

- `data/cleaned_data.csv`  
- `data/preprocessed/...` (16â€¯kHz, preâ€‘emphasized, normalized WAVs)

### 3. Train

Train HMMs:

```bash
# Emotion models
ser train --mode emotion

# Vocalâ€‘channel models (speech vs. song)
ser train --mode vocal
```

Saved under `models/emotion/` and `models/vocal/`.

### 4. Evaluate

Evaluate and display confusion matrices:

```bash
# Emotion recognition
ser eval --mode emotion

# Vocalâ€‘channel
ser eval --mode vocal
```

---

## ğŸ” Methodology

1. **Filename Parsing**:  
   Extract metadata from RAVDESS filenames into `cleaned_data.csv`.

2. **Preprocessing**:  
   Monoâ€‘mix, DCâ€‘offset remove, preâ€‘emphasis, resample to 16â€¯kHz, peak normalize.

3. **Feature Extraction**:  
   MFCCs (13) + delta + deltaâ€‘delta â†’ stacked â†’ CMVN.

4. **HMMs**:  
   - **Emotion**: stratified split by emotion  
   - **Vocal**: leaveâ€‘actorsâ€‘out split  

5. **Evaluation**:  
   Logâ€‘likelihood scoring â†’ confusion matrix & accuracy.

---

## Project Structure

```
speech_emotion_recognition/
â”œâ”€â”€ setup.py
â”œâ”€â”€ config.toml
â”œâ”€â”€ src/
â”‚   â””â”€â”€ speech_emotion_recognition/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ ser.py
â”‚       â”œâ”€â”€ data_download.py
â”‚       â”œâ”€â”€ data_parser.py
â”‚       â”œâ”€â”€ feature_extraction.py
â”‚       â”œâ”€â”€ train.py
â”‚       â””â”€â”€ evaluate.py
â””â”€â”€ data/  (gitâ€‘ignored)
```

---

## Testing

```bash
pip install pytest
pytest
```

---